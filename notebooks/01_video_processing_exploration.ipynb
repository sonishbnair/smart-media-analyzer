{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc3ea671",
   "metadata": {},
   "source": [
    "# Video Processing Exploration \n",
    "\n",
    "## Project: Smart Media Analyzer (Frugal Architecture)\n",
    "\n",
    "### Goals\n",
    "- Test video scene detection with PySceneDetect\n",
    "- Document timing and accuracy for different video types\n",
    "\n",
    "### Current Stack\n",
    "- **Python:** 3.11.13 (UV managed)\n",
    "- **Scene Detection:** PySceneDetect 0.6.6\n",
    "- **Video Processing:** OpenCV 4.11.0\n",
    "\n",
    "### Test Videos\n",
    "- `Giant_Oarfish.mp4` - Nature documentary (~68 seconds)\n",
    "- More videos to be added...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839da5e7",
   "metadata": {},
   "source": [
    "### Video Processing Setup - Testing imports and timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7a8e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Video Processing Setup - Testing imports and timing\n",
    "\"\"\"\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "def log_time(message):\n",
    "    \"\"\"Print message with timestamp for performance tracking\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%H:%M:%S.%f\")[:-3]\n",
    "    print(f\"[{timestamp}] {message}\")\n",
    "\n",
    "# Test imports with timing\n",
    "log_time(\"=== Starting Video Processing Setup ===\")\n",
    "\n",
    "log_time(\"Importing cv2...\")\n",
    "start = time.time()\n",
    "import cv2\n",
    "log_time(f\"cv2 imported in {time.time() - start:.3f}s - Version: {cv2.__version__}\")\n",
    "\n",
    "log_time(\"Importing scenedetect...\")\n",
    "start = time.time()\n",
    "from scenedetect import detect, ContentDetector\n",
    "log_time(f\"scenedetect imported in {time.time() - start:.3f}s\")\n",
    "\n",
    "log_time(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322ca818",
   "metadata": {},
   "source": [
    "### Analyzing video file using OpenCV module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca41e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_video_file(video_path):\n",
    "    \"\"\"Get detailed information about a video file\"\"\"\n",
    "    log_time(f\"Analyzing video file: {video_path}\")\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"ERROR: Video file not found: {video_path}\")\n",
    "        return None\n",
    "    \n",
    "    file_size_mb = os.path.getsize(video_path) / (1024 * 1024)\n",
    "    log_time(f\"File exists - Size: {file_size_mb:.2f} MB\")\n",
    "    \n",
    "    # Get video properties using OpenCV\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"ERROR: Cannot open video with OpenCV\")\n",
    "        return None\n",
    "    \n",
    "    # Extract video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    duration = frame_count / fps if fps > 0 else 0\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Current Time: {datetime.now()}\")\n",
    "    print(f\" Video Properties:\")\n",
    "    print(f\"   Resolution: {width} x {height}\")\n",
    "    print(f\"   FPS: {fps:.2f}\")\n",
    "    print(f\"   Duration: {duration:.2f} seconds\")\n",
    "    print(f\"   Total Frames: {frame_count}\")\n",
    "    print(f\"   File Size: {file_size_mb:.2f} MB\")\n",
    "    \n",
    "    return {\n",
    "        'width': width, 'height': height, 'fps': fps,\n",
    "        'duration': duration, 'frames': frame_count, 'size_mb': file_size_mb\n",
    "    }\n",
    "\n",
    "# Test with your Giant Oarfish video\n",
    "video_file = \"../Samples_Video-Images/Giant_Oarfish.mp4\"\n",
    "print(f\"Analyzing video file: {video_file}\")\n",
    "video_info = analyze_video_file(video_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe60f6c0",
   "metadata": {},
   "source": [
    "### Scene Detection Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65488a8f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde2ce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_scenes_with_analysis(video_path, threshold=27.0):\n",
    "    \"\"\"\n",
    "    Detect scenes and provide detailed analysis\n",
    "    \"\"\"\n",
    "    log_time(f\"Starting scene detection (threshold={threshold})\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Detect scenes using ContentDetector\n",
    "    scene_list = detect(video_path, ContentDetector(threshold=threshold))\n",
    "    \n",
    "    detection_time = time.time() - start_time\n",
    "    log_time(f\"Scene detection completed in {detection_time:.2f}s\")\n",
    "    \n",
    "    # Analysis\n",
    "    total_scenes = len(scene_list)\n",
    "    log_time(f\"Found {total_scenes} scenes\")\n",
    "    \n",
    "    if total_scenes == 0:\n",
    "        print(\"‚ö†Ô∏è  No scenes detected - video might be too uniform\")\n",
    "        return scene_list\n",
    "    \n",
    "    # Calculate scene statistics\n",
    "    durations = []\n",
    "    print(f\"\\nüìä Scene Analysis:\")\n",
    "    print(f\"{'Scene':<8} {'Start':<8} {'End':<8} {'Duration':<10}\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    for i, scene in enumerate(scene_list):\n",
    "        start_sec = scene[0].get_seconds()\n",
    "        end_sec = scene[1].get_seconds()\n",
    "        duration = end_sec - start_sec\n",
    "        durations.append(duration)\n",
    "        \n",
    "        print(f\"{i+1:<8} {start_sec:<8.2f} {end_sec:<8.2f} {duration:<10.2f}\")\n",
    "    \n",
    "    # Statistics\n",
    "    avg_duration = sum(durations) / len(durations)\n",
    "    min_duration = min(durations)\n",
    "    max_duration = max(durations)\n",
    "    \n",
    "    print(f\"\\nüìà Scene Statistics:\")\n",
    "    print(f\"   Total scenes: {total_scenes}\")\n",
    "    print(f\"   Average duration: {avg_duration:.2f}s\")\n",
    "    print(f\"   Shortest scene: {min_duration:.2f}s\")\n",
    "    print(f\"   Longest scene: {max_duration:.2f}s\")\n",
    "    print(f\"   Processing speed: {67.86/detection_time:.1f}x real-time\")\n",
    "    \n",
    "    return scene_list\n",
    "\n",
    "# Run scene detection on your video\n",
    "scenes = detect_scenes_with_analysis(video_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04b8246",
   "metadata": {},
   "source": [
    "### Threshold Experimentation (Code Cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64019167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multiple_videos_with_thresholds():\n",
    "    \"\"\"\n",
    "    Test scene detection with multiple videos and different thresholds\n",
    "    \"\"\"\n",
    "    log_time(\"=== Testing Multiple Video Files with Threshold Analysis ===\")\n",
    "    \n",
    "    # List your available video files here\n",
    "    video_files = [\n",
    "        \"../Samples_Video-Images/Giant_Oarfish.mp4\",\n",
    "        \"../Samples_Video-Images/SoccorShootout_1.mp4\"\n",
    "    ]\n",
    "    \n",
    "    # Check which files exist\n",
    "    available_videos = []\n",
    "    for video_path in video_files:\n",
    "        if os.path.exists(video_path):\n",
    "            available_videos.append(video_path)\n",
    "            print(f\"‚úÖ Found: {video_path}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Missing: {video_path}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    \n",
    "    # Test each video with different thresholds\n",
    "    thresholds = [15.0, 20.0, 27.0, 35.0, 45.0]\n",
    "    all_results = {}\n",
    "    \n",
    "    for video_path in available_videos:\n",
    "        print(f\"\\nüé¨ Analyzing: {os.path.basename(video_path)}\")\n",
    "        \n",
    "        # Get video info first\n",
    "        video_info = analyze_video_file(video_path)\n",
    "        if not video_info:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nüìä Threshold Analysis for {os.path.basename(video_path)}:\")\n",
    "        print(f\"{'Threshold':<12} {'Scenes':<8} {'Avg Duration':<12} {'Time':<8}\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        video_results = []\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            start_time = time.time()\n",
    "            scene_list = detect(video_path, ContentDetector(threshold=threshold))\n",
    "            detection_time = time.time() - start_time\n",
    "            \n",
    "            scene_count = len(scene_list)\n",
    "            avg_duration = video_info['duration'] / scene_count if scene_count > 0 else 0\n",
    "            \n",
    "            video_results.append({\n",
    "                'threshold': threshold,\n",
    "                'scenes': scene_count,\n",
    "                'avg_duration': avg_duration,\n",
    "                'time': detection_time\n",
    "            })\n",
    "            \n",
    "            print(f\"{threshold:<12} {scene_count:<8} {avg_duration:<12.2f} {detection_time:<8.2f}s\")\n",
    "        \n",
    "        all_results[video_path] = {\n",
    "            'info': video_info,\n",
    "            'threshold_results': video_results\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "    \n",
    "    # Comparison summary\n",
    "    if len(available_videos) > 1:\n",
    "        print(f\"\\nüîç Video Comparison Summary (threshold=27.0):\")\n",
    "        print(f\"{'Video':<25} {'Duration':<10} {'Scenes':<8} {'Avg Scene':<10} {'Type':<15}\")\n",
    "        print(\"-\" * 75)\n",
    "        \n",
    "        for video_path in available_videos:\n",
    "            if video_path in all_results:\n",
    "                info = all_results[video_path]['info']\n",
    "                # Find threshold=27.0 result\n",
    "                result_27 = next(r for r in all_results[video_path]['threshold_results'] if r['threshold'] == 27.0)\n",
    "                \n",
    "                video_name = os.path.basename(video_path)[:20]\n",
    "                video_type = \"Nature Doc\" if \"oarfish\" in video_path.lower() else \"Sports\" if \"soccor\" in video_path.lower() else \"Unknown\"\n",
    "                \n",
    "                print(f\"{video_name:<25} {info['duration']:<10.1f} {result_27['scenes']:<8} {result_27['avg_duration']:<10.2f} {video_type:<15}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Run multi-video threshold analysis\n",
    "multi_video_results = test_multiple_videos_with_thresholds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c67892",
   "metadata": {},
   "source": [
    "### Frame Extraction Function\n",
    "Extract the frames from the above scene detection logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe2784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Configure matplotlib for better display\n",
    "plt.rcParams['figure.figsize'] = [15, 8]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "def extract_keyframes_from_scenes(video_path, scene_list, frames_per_scene=1, frame_position='middle'):\n",
    "    \"\"\"\n",
    "    Extract keyframes from detected scenes\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to video file\n",
    "        scene_list: List of scenes from PySceneDetect\n",
    "        frames_per_scene: Number of frames to extract per scene (1, 2, or 3)\n",
    "        frame_position: 'start', 'middle', 'end', or 'all'\n",
    "    \n",
    "    Returns:\n",
    "        List of frame data with metadata\n",
    "    \"\"\"\n",
    "    log_time(f\"Extracting keyframes from {len(scene_list)} scenes...\")\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"‚ùå Cannot open video: {video_path}\")\n",
    "        return []\n",
    "    \n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames_extracted = 0\n",
    "    extracted_frames = []\n",
    "    \n",
    "    for scene_idx, scene in enumerate(scene_list):\n",
    "        scene_start_sec = scene[0].get_seconds()\n",
    "        scene_end_sec = scene[1].get_seconds()\n",
    "        scene_duration = scene_end_sec - scene_start_sec\n",
    "        \n",
    "        # Calculate frame positions based on preference\n",
    "        frame_times = []\n",
    "        \n",
    "        if frame_position == 'start':\n",
    "            frame_times = [scene_start_sec + 0.1]  # 0.1s after start\n",
    "        elif frame_position == 'end':\n",
    "            frame_times = [scene_end_sec - 0.1]    # 0.1s before end\n",
    "        elif frame_position == 'middle':\n",
    "            frame_times = [scene_start_sec + (scene_duration / 2)]\n",
    "        elif frame_position == 'all' and frames_per_scene == 3:\n",
    "            frame_times = [\n",
    "                scene_start_sec + (scene_duration * 0.2),  # 20% into scene\n",
    "                scene_start_sec + (scene_duration * 0.5),  # 50% into scene  \n",
    "                scene_start_sec + (scene_duration * 0.8)   # 80% into scene\n",
    "            ]\n",
    "        \n",
    "        # Extract frames at calculated times\n",
    "        for frame_idx, frame_time in enumerate(frame_times):\n",
    "            frame_number = int(frame_time * fps)\n",
    "            \n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if ret:\n",
    "                # Convert BGR to RGB for matplotlib\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                extracted_frames.append({\n",
    "                    'scene_number': scene_idx + 1,\n",
    "                    'frame_index': frame_idx + 1,\n",
    "                    'timestamp': frame_time,\n",
    "                    'scene_start': scene_start_sec,\n",
    "                    'scene_end': scene_end_sec,\n",
    "                    'scene_duration': scene_duration,\n",
    "                    'frame_number': frame_number,\n",
    "                    'frame_data': frame_rgb,\n",
    "                    'frame_shape': frame_rgb.shape\n",
    "                })\n",
    "                \n",
    "                total_frames_extracted += 1\n",
    "    \n",
    "    cap.release()\n",
    "    log_time(f\"‚úÖ Extracted {total_frames_extracted} keyframes from {len(scene_list)} scenes\")\n",
    "    \n",
    "    return extracted_frames\n",
    "\n",
    "def visualize_extracted_keyframes(extracted_frames, max_display=10):\n",
    "    \"\"\"\n",
    "    Display extracted keyframes in a grid\n",
    "    \"\"\"\n",
    "    if not extracted_frames:\n",
    "        print(\"‚ùå No frames to display\")\n",
    "        return\n",
    "    \n",
    "    frames_to_show = min(max_display, len(extracted_frames))\n",
    "    log_time(f\"Displaying first {frames_to_show} keyframes...\")\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    cols = 3 if frames_to_show > 6 else 2 if frames_to_show > 2 else 1\n",
    "    rows = (frames_to_show + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "    if rows == 1 and cols == 1:\n",
    "        axes = [axes]\n",
    "    elif rows == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i in range(frames_to_show):\n",
    "        frame_info = extracted_frames[i]\n",
    "        \n",
    "        axes[i].imshow(frame_info['frame_data'])\n",
    "        axes[i].set_title(\n",
    "            f\"Scene {frame_info['scene_number']} - {frame_info['timestamp']:.2f}s\\n\"\n",
    "            f\"Duration: {frame_info['scene_duration']:.2f}s\", \n",
    "            fontsize=10\n",
    "        )\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(frames_to_show, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def save_keyframes_metadata(extracted_frames, output_file=\"keyframes_metadata.json\"):\n",
    "    \"\"\"\n",
    "    Save keyframe metadata to JSON file (without actual image data)\n",
    "    \"\"\"\n",
    "    import json\n",
    "    \n",
    "    metadata = []\n",
    "    for frame_info in extracted_frames:\n",
    "        metadata.append({\n",
    "            'scene_number': frame_info['scene_number'],\n",
    "            'frame_index': frame_info['frame_index'],\n",
    "            'timestamp': frame_info['timestamp'],\n",
    "            'scene_start': frame_info['scene_start'],\n",
    "            'scene_end': frame_info['scene_end'],\n",
    "            'scene_duration': frame_info['scene_duration'],\n",
    "            'frame_number': frame_info['frame_number'],\n",
    "            'frame_shape': frame_info['frame_shape']\n",
    "        })\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    log_time(f\"üìÑ Saved metadata for {len(metadata)} frames to {output_file}\")\n",
    "\n",
    "# Test frame extraction with your video\n",
    "log_time(\"=== Testing Frame Extraction ===\")\n",
    "\n",
    "# Extract middle frame from each scene for Giant Oarfish video\n",
    "oarfish_frames = extract_keyframes_from_scenes(\n",
    "    \"../Samples_Video-Images/Giant_Oarfish.mp4\", \n",
    "    scenes,  # Using scenes from previous cell\n",
    "    frames_per_scene=1,\n",
    "    frame_position='middle'\n",
    ")\n",
    "\n",
    "# Display the keyframes\n",
    "visualize_extracted_keyframes(oarfish_frames, max_display=3)\n",
    "\n",
    "# Save metadata\n",
    "save_keyframes_metadata(oarfish_frames, \"giant_oarfish_keyframes.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707fa353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "365ebff5",
   "metadata": {},
   "source": [
    "### OpenCLIP Exploration\n",
    "https://openai.com/index/clip/\n",
    "#### Test OpenCLIP installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca09175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test OpenCLIP installation\n",
    "try:\n",
    "    import open_clip\n",
    "    import torch\n",
    "    \n",
    "    print(f\"‚úÖ OpenCLIP imported successfully\")\n",
    "    print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "    print(f\"‚úÖ MPS (M1 GPU) available: {torch.backends.mps.is_available()}\")\n",
    "    \n",
    "    # List available models\n",
    "    available_models = open_clip.list_pretrained()\n",
    "    print(f\"‚úÖ Available models: {len(available_models)} models found\")\n",
    "    print(\"First few models:\", available_models[:3])\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b463616",
   "metadata": {},
   "source": [
    "#### Basic OpenCLIP; first frame demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4769d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "# Load a lightweight model optimized for M1\n",
    "model, preprocess = open_clip.create_model_from_pretrained(\n",
    "    'ViT-B-32', \n",
    "    pretrained='openai',\n",
    "    device='mps' if torch.backends.mps.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "\n",
    "log_time(\"‚úÖ OpenCLIP model loaded successfully\")\n",
    "\n",
    "# Test with one of your extracted keyframes\n",
    "def analyze_keyframe_with_openclip(frame_data, text_queries):\n",
    "    \"\"\"\n",
    "    Analyze a keyframe using OpenCLIP\n",
    "    \"\"\"\n",
    "    # Convert your extracted frame to PIL Image\n",
    "    pil_image = Image.fromarray(frame_data)\n",
    "    \n",
    "    # Preprocess image\n",
    "    image_input = preprocess(pil_image).unsqueeze(0)\n",
    "    if torch.backends.mps.is_available():\n",
    "        image_input = image_input.to('mps')\n",
    "    \n",
    "    # Tokenize text queries\n",
    "    text_input = tokenizer(text_queries)\n",
    "    if torch.backends.mps.is_available():\n",
    "        text_input = text_input.to('mps')\n",
    "    \n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input)\n",
    "        text_features = model.encode_text(text_input)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = (image_features @ text_features.T).softmax(dim=-1)\n",
    "    \n",
    "    return similarities.cpu().numpy()[0]\n",
    "\n",
    "# Test with your first keyframe\n",
    "if 'oarfish_frames' in globals() and len(oarfish_frames) > 0:\n",
    "    test_queries = [\n",
    "        \"a large fish swimming underwater\",\n",
    "        \"marine life in the ocean\", \n",
    "        \"underwater scene\",\n",
    "        \"a person on land\",\n",
    "        \"a car driving\"\n",
    "    ]\n",
    "    \n",
    "    similarities = analyze_keyframe_with_openclip(\n",
    "        oarfish_frames[0]['frame_data'], \n",
    "        test_queries\n",
    "    )\n",
    "    \n",
    "    print(\"üéØ Similarity Scores for First Keyframe:\")\n",
    "    for query, score in zip(test_queries, similarities):\n",
    "        print(f\"   {query}: {score:.3f}\")\n",
    "        \n",
    "    # Find best match\n",
    "    best_match_idx = np.argmax(similarities)\n",
    "    print(f\"\\nüèÜ Best match: '{test_queries[best_match_idx]}' (score: {similarities[best_match_idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb059b6",
   "metadata": {},
   "source": [
    "#### Analyze first few frames using OpenCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1d3fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with first 5 keyframes\n",
    "if 'oarfish_frames' in globals() and len(oarfish_frames) > 0:\n",
    "    test_queries = [\n",
    "        \"a large fish swimming underwater\",\n",
    "        \"marine life in the ocean\", \n",
    "        \"underwater scene\",\n",
    "        \"a person on land\",\n",
    "        \"a car driving\"\n",
    "    ]\n",
    "    \n",
    "    total_frames = len(oarfish_frames)\n",
    "    frames_to_analyze = min(5, total_frames)  # In case there are fewer than 5 frames\n",
    "    \n",
    "    print(f\"üéØ Analyzing first 5 frames from {total_frames} total frames:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i in range(frames_to_analyze):\n",
    "        frame_info = oarfish_frames[i]\n",
    "        \n",
    "        # Analyze this frame\n",
    "        similarities = analyze_keyframe_with_openclip(\n",
    "            frame_info['frame_data'], \n",
    "            test_queries\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüìã Frame {i+1}/5 - Scene {frame_info['scene_number']} at {frame_info['timestamp']:.2f}s:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Show all similarity scores\n",
    "        for query, score in zip(test_queries, similarities):\n",
    "            print(f\"   {query}: {score:.3f}\")\n",
    "        \n",
    "        # Find and highlight best match\n",
    "        best_match_idx = np.argmax(similarities)\n",
    "        best_score = similarities[best_match_idx]\n",
    "        print(f\"   üèÜ Best: '{test_queries[best_match_idx]}' ({best_score:.3f})\")\n",
    "        \n",
    "        # Show confidence level\n",
    "        if best_score > 0.7:\n",
    "            confidence = \"üü¢ High confidence\"\n",
    "        elif best_score > 0.5:\n",
    "            confidence = \"üü° Medium confidence\"\n",
    "        else:\n",
    "            confidence = \"üî¥ Low confidence\"\n",
    "        print(f\"   {confidence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4232a4d6",
   "metadata": {},
   "source": [
    "#### OpenCLIP Comprehensive Analysis\n",
    "\n",
    "Creates category vocabulary - Defines 50+ visual concepts across marine life, environments, objects, actions, and visual properties.\n",
    "\n",
    "Analyzes keyframes - Compares each frame against all categories using AI vision-language understanding.\n",
    "\n",
    "Returns structured metadata - Provides confidence scores for detected features (e.g., \"large fish: 0.89\", \"deep sea: 0.78\").\n",
    "\n",
    "Handles edge cases - Uses fallback classification for low-confidence frames to ensure every frame gets some description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c64853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_categories():\n",
    "    \"\"\"\n",
    "    Create hierarchical categories covering visual content\n",
    "    TODO: Expand these category lists for more comprehensive coverage\n",
    "    \"\"\"\n",
    "    categories = {\n",
    "        # LIVING THINGS\n",
    "        \"marine_animals\": [\n",
    "            \"small fish\", \"large fish\", \"school of fish\", \"shark\", \"whale\", \n",
    "            \"dolphin\", \"octopus\", \"jellyfish\", \"sea turtle\", \"ray\", \"eel\",\n",
    "            \"marine animal\", \"sea creature\", \"underwater animal\"\n",
    "        ],\n",
    "        \n",
    "        \"land_animals\": [\n",
    "            \"bird\", \"mammal\", \"reptile\", \"domestic animal\", \"wild animal\"\n",
    "        ],\n",
    "        \n",
    "        \"humans\": [\n",
    "            \"person\", \"people\", \"diver\", \"swimmer\", \"child\", \"adult\"\n",
    "        ],\n",
    "        \n",
    "        # ENVIRONMENTS  \n",
    "        \"water_environments\": [\n",
    "            \"ocean\", \"deep sea\", \"shallow water\", \"coral reef\", \"open water\",\n",
    "            \"underwater cave\", \"clear water\", \"murky water\"\n",
    "        ],\n",
    "        \n",
    "        \"land_environments\": [\n",
    "            \"beach\", \"forest\", \"desert\", \"mountain\", \"urban area\", \"indoor space\"\n",
    "        ],\n",
    "        \n",
    "        # OBJECTS & ACTIONS\n",
    "        \"objects\": [\n",
    "            \"boat\", \"ship\", \"rock\", \"coral\", \"seaweed\", \"equipment\"\n",
    "        ],\n",
    "        \n",
    "        \"movements\": [\n",
    "            \"swimming\", \"floating\", \"diving\", \"resting\", \"feeding\", \"hunting\"\n",
    "        ],\n",
    "        \n",
    "        # VISUAL PROPERTIES\n",
    "        \"image_composition\": [\n",
    "            \"close-up\", \"wide shot\", \"medium shot\", \"underwater view\"\n",
    "        ],\n",
    "        \n",
    "        \"lighting_conditions\": [\n",
    "            \"bright light\", \"dim light\", \"underwater lighting\", \"natural lighting\"\n",
    "        ],\n",
    "        \n",
    "        # GENERAL CATEGORIES (fallback)\n",
    "        \"general_content\": [\n",
    "            \"natural scene\", \"calm scene\", \"interesting content\", \"documentary style\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return categories\n",
    "\n",
    "def analyze_frame_with_fallback(frame_data, confidence_threshold=0.15):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis with fallback for unmatched content\n",
    "    \"\"\"\n",
    "    categories = create_comprehensive_categories()\n",
    "    \n",
    "    pil_image = Image.fromarray(frame_data)\n",
    "    image_input = preprocess(pil_image).unsqueeze(0)\n",
    "    if torch.backends.mps.is_available():\n",
    "        image_input = image_input.to('mps')\n",
    "    \n",
    "    results = {}\n",
    "    all_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input)\n",
    "        \n",
    "        # Analyze each category group\n",
    "        for category_name, items in categories.items():\n",
    "            text_input = tokenizer(items)\n",
    "            if torch.backends.mps.is_available():\n",
    "                text_input = text_input.to('mps')\n",
    "                \n",
    "            text_features = model.encode_text(text_input)\n",
    "            similarities = (image_features @ text_features.T).softmax(dim=-1)\n",
    "            \n",
    "            # Get matches above threshold\n",
    "            high_confidence_items = []\n",
    "            for i, score in enumerate(similarities[0]):\n",
    "                if score > confidence_threshold:\n",
    "                    high_confidence_items.append({\n",
    "                        \"label\": items[i],\n",
    "                        \"confidence\": score.item()\n",
    "                    })\n",
    "                    all_confidences.append(score.item())\n",
    "            \n",
    "            if high_confidence_items:\n",
    "                # Sort by confidence and keep top 3\n",
    "                high_confidence_items.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "                results[category_name] = high_confidence_items[:3]\n",
    "    \n",
    "    # FALLBACK for low-confidence frames\n",
    "    max_confidence = max(all_confidences) if all_confidences else 0\n",
    "    \n",
    "    if max_confidence < 0.3:\n",
    "        fallback_categories = [\n",
    "            \"visual content\", \"video frame\", \"general scene\", \"recorded material\"\n",
    "        ]\n",
    "        \n",
    "        text_input = tokenizer(fallback_categories)\n",
    "        if torch.backends.mps.is_available():\n",
    "            text_input = text_input.to('mps')\n",
    "            \n",
    "        text_features = model.encode_text(text_input)\n",
    "        similarities = (image_features @ text_features.T).softmax(dim=-1)\n",
    "        \n",
    "        best_fallback = similarities[0].argmax()\n",
    "        results[\"fallback_classification\"] = {\n",
    "            \"label\": fallback_categories[best_fallback],\n",
    "            \"confidence\": similarities[0][best_fallback].item(),\n",
    "            \"note\": \"Low confidence - using generic classification\"\n",
    "        }\n",
    "    \n",
    "    # Add analysis metadata\n",
    "    total_categories = sum(len(items) for items in categories.values())\n",
    "    results[\"analysis_metadata\"] = {\n",
    "        \"total_categories_tested\": total_categories,\n",
    "        \"max_confidence\": max_confidence,\n",
    "        \"avg_confidence\": sum(all_confidences) / len(all_confidences) if all_confidences else 0,\n",
    "        \"features_detected\": len(all_confidences),\n",
    "        \"analysis_quality\": \"high\" if max_confidence > 0.7 else \"medium\" if max_confidence > 0.4 else \"low\"\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with first keyframe\n",
    "if 'oarfish_frames' in globals() and len(oarfish_frames) > 0:\n",
    "    log_time(\"=== Starting Comprehensive OpenCLIP Analysis ===\")\n",
    "    \n",
    "    # Analyze first frame\n",
    "    frame_info = oarfish_frames[0]\n",
    "    result = analyze_frame_with_fallback(\n",
    "        frame_info['frame_data'], \n",
    "        confidence_threshold=0.20\n",
    "    )\n",
    "    \n",
    "    print(\"üéØ Comprehensive Analysis Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # DISPLAY FRAME IMAGE FIRST\n",
    "    print(f\"\\nüñºÔ∏è  ANALYZING FRAME:\")\n",
    "    print(f\"   Scene {frame_info['scene_number']} at {frame_info['timestamp']:.2f}s\")\n",
    "    \n",
    "    # Show the image\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(frame_info['frame_data'])\n",
    "    plt.title(f\"Scene {frame_info['scene_number']} - Keyframe at {frame_info['timestamp']:.2f}s\", \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Then show analysis results\n",
    "    for category, items in result.items():\n",
    "        if category != \"analysis_metadata\" and category != \"fallback_classification\":\n",
    "            print(f\"\\nüìã {category.upper().replace('_', ' ')}:\")\n",
    "            for item in items:\n",
    "                confidence_icon = \"üü¢\" if item['confidence'] > 0.7 else \"üü°\" if item['confidence'] > 0.4 else \"üî¥\"\n",
    "                print(f\"   {confidence_icon} {item['label']}: {item['confidence']:.3f}\")\n",
    "    \n",
    "    # Show fallback if present\n",
    "    if \"fallback_classification\" in result:\n",
    "        print(f\"\\n‚ö†Ô∏è  FALLBACK:\")\n",
    "        fallback = result[\"fallback_classification\"]\n",
    "        print(f\"   {fallback['label']}: {fallback['confidence']:.3f}\")\n",
    "        print(f\"   Note: {fallback['note']}\")\n",
    "    \n",
    "    # Show metadata\n",
    "    print(f\"\\nüìä ANALYSIS SUMMARY:\")\n",
    "    metadata = result[\"analysis_metadata\"]\n",
    "    print(f\"   ‚Ä¢ Quality: {metadata['analysis_quality'].upper()}\")\n",
    "    print(f\"   ‚Ä¢ Best confidence: {metadata['max_confidence']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Features detected: {metadata['features_detected']}\")\n",
    "    print(f\"   ‚Ä¢ Categories tested: {metadata['total_categories_tested']}\")\n",
    "    \n",
    "    log_time(\"‚úÖ Comprehensive analysis completed\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå oarfish_frames not found. Please run frame extraction first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e4d24d",
   "metadata": {},
   "source": [
    "#### Analyzing multiple frames, show extarcted output; only show categories where confidence is > 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7a62ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with first 5 keyframes\n",
    "if 'oarfish_frames' in globals() and len(oarfish_frames) > 0:\n",
    "    log_time(\"=== Starting Comprehensive OpenCLIP Analysis ===\")\n",
    "    \n",
    "    frames_to_analyze = min(5, len(oarfish_frames))  # Analyze up to 5 frames\n",
    "    \n",
    "    print(\"üéØ Comprehensive Analysis Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for frame_idx in range(frames_to_analyze):\n",
    "        frame_info = oarfish_frames[frame_idx]\n",
    "        \n",
    "        # Analyze current frame\n",
    "        result = analyze_frame_with_fallback(\n",
    "            frame_info['frame_data'], \n",
    "            confidence_threshold=0.20\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüñºÔ∏è  FRAME {frame_idx + 1}/5:\")\n",
    "        print(f\"   Scene {frame_info['scene_number']} at {frame_info['timestamp']:.2f}s\")\n",
    "        print(f\"   Duration: {frame_info['scene_duration']:.2f}s\")\n",
    "        \n",
    "        # Show the image using plt.figure\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.imshow(frame_info['frame_data'])\n",
    "        plt.title(f\"Frame {frame_idx + 1} - Scene {frame_info['scene_number']} at {frame_info['timestamp']:.2f}s\", \n",
    "                  fontsize=12, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Show analysis results for this frame - ONLY confidence > 0.7\n",
    "        high_confidence_found = False\n",
    "        for category, items in result.items():\n",
    "            if category != \"analysis_metadata\" and category != \"fallback_classification\":\n",
    "                # Filter items with confidence > 0.7\n",
    "                high_confidence_items = [item for item in items if item['confidence'] > 0.7]\n",
    "                \n",
    "                if high_confidence_items:\n",
    "                    if not high_confidence_found:  # First high confidence category\n",
    "                        high_confidence_found = True\n",
    "                    \n",
    "                    print(f\"\\nüìã {category.upper().replace('_', ' ')} (High Confidence):\")\n",
    "                    for item in high_confidence_items:\n",
    "                        print(f\"   üü¢ {item['label']}: {item['confidence']:.3f}\")\n",
    "        \n",
    "        # If no high confidence results found, show a message\n",
    "        if not high_confidence_found:\n",
    "            print(f\"\\n‚ö†Ô∏è  No high confidence results (>0.7) found for this frame\")\n",
    "            # Show top result from metadata\n",
    "            metadata = result[\"analysis_metadata\"]\n",
    "            print(f\"   Highest confidence achieved: {metadata['max_confidence']:.3f}\")\n",
    "        \n",
    "        # Show fallback if present\n",
    "        if \"fallback_classification\" in result:\n",
    "            fallback = result[\"fallback_classification\"]\n",
    "            if fallback['confidence'] > 0.7:\n",
    "                print(f\"\\n‚ö†Ô∏è  FALLBACK (High Confidence):\")\n",
    "                print(f\"   üü¢ {fallback['label']}: {fallback['confidence']:.3f}\")\n",
    "                print(f\"   Note: {fallback['note']}\")\n",
    "        \n",
    "        # Show metadata summary\n",
    "        metadata = result[\"analysis_metadata\"]\n",
    "        print(f\"\\nüìä FRAME {frame_idx + 1} SUMMARY:\")\n",
    "        print(f\"   ‚Ä¢ Quality: {metadata['analysis_quality'].upper()}\")\n",
    "        print(f\"   ‚Ä¢ Best confidence: {metadata['max_confidence']:.3f}\")\n",
    "        print(f\"   ‚Ä¢ High confidence features (>0.7): {sum(1 for category, items in result.items() if category not in ['analysis_metadata', 'fallback_classification'] for item in items if item['confidence'] > 0.7)}\")\n",
    "        \n",
    "        # Add separator between frames\n",
    "        if frame_idx < frames_to_analyze - 1:  # Don't add separator after last frame\n",
    "            print(\"\\n\" + \"‚îÄ\" * 60)\n",
    "    \n",
    "    log_time(\"‚úÖ Comprehensive analysis of 5 frames completed\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå oarfish_frames not found. Please run frame extraction first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ef29ad",
   "metadata": {},
   "source": [
    "=============================\n",
    "### Database Checks for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d345d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to Python path\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Now import should work\n",
    "from database import print_dependency_status, check_dependencies\n",
    "\n",
    "print_dependency_status()\n",
    "deps = check_dependencies()\n",
    "print(f\"All required deps available: {all(deps.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a1bdb3",
   "metadata": {},
   "source": [
    "### Initialize database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e54c1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup (same as before)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Create database - now will create in database/databases/\n",
    "from database import MediaArchiveDB\n",
    "db = MediaArchiveDB()\n",
    "\n",
    "print(f\"‚úÖ Database created at: {db.db_path}\")\n",
    "print(f\"üìÅ Full path: {Path(db.db_path).absolute()}\")\n",
    "\n",
    "# Verify the folder structure\n",
    "import os\n",
    "print(f\"Database file exists: {os.path.exists(db.db_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7113dffb",
   "metadata": {},
   "source": [
    "### Test with your existing video data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d680e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your notebook, test the new path\n",
    "success, video_id, error = db.add_video({\n",
    "    'filepath': '../Samples_Video-Images/Giant_Oarfish.mp4',\n",
    "    'filename': 'Giant_Oarfish.mp4',\n",
    "    'duration_seconds': 67.86,\n",
    "    'fps': 23.98,\n",
    "    'width': 1280,\n",
    "    'height': 720,\n",
    "    'total_frames': 1627,\n",
    "    'file_size_mb': 11.76\n",
    "})\n",
    "\n",
    "print(f\"Add video result: Success={success}, ID={video_id}\")\n",
    "print(f\"Database location: {db.db_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smart-media-analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
